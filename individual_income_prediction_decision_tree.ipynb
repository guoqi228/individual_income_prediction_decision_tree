{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Income Prediction Using Decision Tree\n",
    "\n",
    "#### In this project, we'll be looking at individual income in the United States. The data is from the 1994 census, and contains information on an individual's marital status, age, type of work, and more. The target column, or what we want to predict, is whether individuals make less than or equal to 50k a year, or more than 50k a year. The data from the University of California, Irvine's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>high_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education_num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital_status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country high_income  \n",
       "0          2174             0              40   United-States       <=50K  \n",
       "1             0             0              13   United-States       <=50K  \n",
       "2             0             0              40   United-States       <=50K  \n",
       "3             0             0              40   United-States       <=50K  \n",
       "4             0             0              40            Cuba       <=50K  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries and data\n",
    "import pandas\n",
    "# Set index_col to False to avoid pandas thinking that the first column is row indexes (it's age)\n",
    "income = pandas.read_csv(\"income.csv\", index_col=False)\n",
    "income.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age               32561 non-null int64\n",
      "workclass         32561 non-null object\n",
      "fnlwgt            32561 non-null int64\n",
      "education         32561 non-null object\n",
      "education_num     32561 non-null int64\n",
      "marital_status    32561 non-null object\n",
      "occupation        32561 non-null object\n",
      "relationship      32561 non-null object\n",
      "race              32561 non-null object\n",
      "sex               32561 non-null object\n",
      "capital_gain      32561 non-null int64\n",
      "capital_loss      32561 non-null int64\n",
      "hours_per_week    32561 non-null int64\n",
      "native_country    32561 non-null object\n",
      "high_income       32561 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "income.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical data into intgeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Never-married'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income['marital_status'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14976\n",
       "1    10683\n",
       "2     4443\n",
       "3     1025\n",
       "4      993\n",
       "5      418\n",
       "6       23\n",
       "Name: marital_status, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marital_mapper = {' Married-civ-spouse': 0,\n",
    "                 ' Never-married': 1,\n",
    "                 ' Divorced': 2,\n",
    "                 ' Separated': 3,\n",
    "                 ' Widowed': 4,\n",
    "                 ' Married-spouse-absent': 5,\n",
    "                 ' Married-AF-spouse': 6}\n",
    "income['marital_status'] = income['marital_status'].map(marital_mapper)\n",
    "income['marital_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22696\n",
       "1     2541\n",
       "2     2093\n",
       "3     1836\n",
       "4     1298\n",
       "5     1116\n",
       "6      960\n",
       "7       14\n",
       "8        7\n",
       "Name: workclass, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workclass_mapper = {' Private': 0,\n",
    "                   ' Self-emp-not-inc': 1,\n",
    "                   ' Local-gov': 2,\n",
    "                   ' ?': 3,\n",
    "                   ' State-gov': 4,\n",
    "                   ' Self-emp-inc': 5,\n",
    "                   ' Federal-gov': 6,\n",
    "                   ' Without-pay': 7,\n",
    "                   ' Never-worked': 8}\n",
    "income['workclass'] = income['workclass'].map(workclass_mapper)\n",
    "income['workclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     4140\n",
       "1     4099\n",
       "2     4066\n",
       "3     3770\n",
       "4     3650\n",
       "5     3295\n",
       "6     2002\n",
       "7     1843\n",
       "8     1597\n",
       "9     1370\n",
       "10     994\n",
       "11     928\n",
       "12     649\n",
       "13     149\n",
       "14       9\n",
       "Name: occupation, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupation_mapper = {' Prof-specialty': 0,\n",
    "                     ' Craft-repair': 1,     \n",
    "                     ' Exec-managerial': 2,  \n",
    "                     ' Adm-clerical': 3,     \n",
    "                     ' Sales': 4,            \n",
    "                     ' Other-service':5,    \n",
    "                     ' Machine-op-inspct':6,\n",
    "                     ' ?': 7,                \n",
    "                     ' Transport-moving': 8, \n",
    "                     ' Handlers-cleaners': 9,\n",
    "                     ' Farming-fishing': 10,  \n",
    "                     ' Tech-support': 11,     \n",
    "                     ' Protective-serv': 12,  \n",
    "                     ' Priv-house-serv': 13,  \n",
    "                     ' Armed-Forces': 14}\n",
    "income['occupation'] = income['occupation'].map(occupation_mapper)\n",
    "income['occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13193\n",
       "1     8305\n",
       "2     5068\n",
       "3     3446\n",
       "4     1568\n",
       "5      981\n",
       "Name: relationship, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationship_mapper = { ' Husband': 0,\n",
    "                         ' Not-in-family': 1,\n",
    "                         ' Own-child': 2,\n",
    "                         ' Unmarried': 3,\n",
    "                         ' Wife': 4,\n",
    "                         ' Other-relative': 5}\n",
    "income['relationship'] = income['relationship'].map(relationship_mapper)\n",
    "income['relationship'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27816\n",
       "1     3124\n",
       "2     1039\n",
       "3      311\n",
       "4      271\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_mapper = {  ' White': 0,\n",
    "                 ' Black': 1,\n",
    "                 ' Asian-Pac-Islander': 2,\n",
    "                 ' Amer-Indian-Eskimo': 3,\n",
    "                 ' Other'             : 4}\n",
    "income['race'] = income['race'].map(race_mapper)\n",
    "income['race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21790\n",
       "1    10771\n",
       "Name: sex, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sex_mapper = {' Male': 0, ' Female': 1}\n",
    "income['sex'] = income['sex'].map(sex_mapper)\n",
    "income['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     <=50K\n",
       "1     <=50K\n",
       "2     <=50K\n",
       "3     <=50K\n",
       "4     <=50K\n",
       "5     <=50K\n",
       "6     <=50K\n",
       "7      >50K\n",
       "8      >50K\n",
       "9      >50K\n",
       "Name: high_income, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income['high_income'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24720\n",
       "1     7841\n",
       "Name: high_income, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_income_mapper = {' >50K': 1, ' <=50K': 0}\n",
    "income['high_income'] = income['high_income'].map(high_income_mapper)\n",
    "income['high_income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' United-States': 0, ' Mexico': 1, ' ?': 2, ' Philippines': 3, ' Germany': 4, ' Canada': 5, ' Puerto-Rico': 6, ' El-Salvador': 7, ' India': 8, ' Cuba': 9, ' England': 10, ' Jamaica': 11, ' South': 12, ' China': 13, ' Italy': 14, ' Dominican-Republic': 15, ' Vietnam': 16, ' Guatemala': 17, ' Japan': 18, ' Poland': 19, ' Columbia': 20, ' Taiwan': 21, ' Haiti': 22, ' Iran': 23, ' Portugal': 24, ' Nicaragua': 25, ' Peru': 26, ' Greece': 27, ' France': 28, ' Ecuador': 29, ' Ireland': 30, ' Hong': 31, ' Cambodia': 32, ' Trinadad&Tobago': 33, ' Thailand': 34, ' Laos': 35, ' Yugoslavia': 36, ' Outlying-US(Guam-USVI-etc)': 37, ' Honduras': 38, ' Hungary': 39, ' Scotland': 40, ' Holand-Netherlands': 41}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     29170\n",
       "1       643\n",
       "2       583\n",
       "3       198\n",
       "4       137\n",
       "5       121\n",
       "6       114\n",
       "7       106\n",
       "8       100\n",
       "9        95\n",
       "10       90\n",
       "11       81\n",
       "12       80\n",
       "13       75\n",
       "14       73\n",
       "15       70\n",
       "16       67\n",
       "17       64\n",
       "18       62\n",
       "19       60\n",
       "20       59\n",
       "21       51\n",
       "22       44\n",
       "23       43\n",
       "24       37\n",
       "25       34\n",
       "26       31\n",
       "27       29\n",
       "28       29\n",
       "29       28\n",
       "30       24\n",
       "31       20\n",
       "33       19\n",
       "32       19\n",
       "35       18\n",
       "34       18\n",
       "36       16\n",
       "37       14\n",
       "39       13\n",
       "38       13\n",
       "40       12\n",
       "41        1\n",
       "Name: native_country, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "native_country_counts = income['native_country'].value_counts()\n",
    "native_country_mapper = {}\n",
    "for i, name in enumerate(native_country_counts.index):\n",
    "    native_country_mapper[name] = i\n",
    "print(native_country_mapper)\n",
    "income['native_country'] = income['native_country'].map(native_country_mapper)\n",
    "income['native_country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age               32561 non-null int64\n",
      "workclass         32561 non-null int64\n",
      "fnlwgt            32561 non-null int64\n",
      "education         32561 non-null object\n",
      "education_num     32561 non-null int64\n",
      "marital_status    32561 non-null int64\n",
      "occupation        32561 non-null int64\n",
      "relationship      32561 non-null int64\n",
      "race              32561 non-null int64\n",
      "sex               32561 non-null int64\n",
      "capital_gain      32561 non-null int64\n",
      "capital_loss      32561 non-null int64\n",
      "hours_per_week    32561 non-null int64\n",
      "native_country    32561 non-null int64\n",
      "high_income       32561 non-null int64\n",
      "dtypes: int64(14), object(1)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "income.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can split the data set into two portions based on whether the individual works in the private sector or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1298, 15)\n",
      "(31263, 15)\n"
     ]
    }
   ],
   "source": [
    "# private_incomes should contain all rows where workclass is 4.\n",
    "# public_incomes should contain all rows where workclass is not 4.\n",
    "private_incomes = income[income[\"workclass\"] == 4]\n",
    "public_incomes = income[income[\"workclass\"] != 4]\n",
    "print(private_incomes.shape)\n",
    "print(public_incomes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When we performed the split, 9865 rows went to the left, where workclass does not equal 4, and 22696 rows went to the right, where workclass equals 4.\n",
    "\n",
    "#### It's useful to think of a decision tree as a flow of rows of data. When we make a split, some rows will go to the right, and some will go to the left. As we build the tree deeper and deeper, each node will \"receive\" fewer and fewer rows.\n",
    "\n",
    "#### The nodes at the bottom of the tree, where we decide to stop splitting, are called terminal nodes, or leaves. When we do our splits, we aren't doing them randomly; we have an objective. Our goal is to ensure that we can make a prediction on future data. In order to do this, all rows in each leaf must have only one value for our target column.\n",
    "\n",
    "#### We're trying to predict the high_income column.\n",
    "- If high_income is 1, it means that the person has an income higher than 50k a year.\n",
    "- If high_income is 0, it means that they have an income less than or equal to 50k a year.\n",
    "\n",
    "#### After constructing a tree using the income data, we'll want to make predictions. In order to do this, we'll take a new row and feed it through our decision tree.\n",
    "\n",
    "#### First, we check whether the person works in the private sector. If they do, we'll check to see whether they're native to the US, and so on.\n",
    "\n",
    "#### Eventually, we'll reach a leaf. The leaf will tell us what value we should predict for high_income.\n",
    "\n",
    "#### In order to be able to make this prediction, all of the rows in a leaf should only have a single value for high_income. This means that leaves can't have both 0 and 1 values in the high_income column. Each leaf can only have rows with the same values for our target column. If this isn't the case, we won't be able to make effective predictions.\n",
    "\n",
    "#### We'll need to continue splitting nodes until we get to a point where all of the rows in a node have the same value for high_income.\n",
    "\n",
    "#### Now that we have a high-level view of how decision trees work, let's explore the details and learn how to perform the splits.\n",
    "\n",
    "#### We'll use a specific measure to figure out which variables we should split nodes on. Post-split, we'll have two data sets, each containing the rows from one branch of the split.\n",
    "\n",
    "#### Because we're trying to reach the leaves having only 1s or only 0s in high_income, each split will need to get us closer to that goal.\n",
    "\n",
    "#### When we split, we'll try to separate as many 0s from 1s in the high_income column as we can. In order to do this, we need a metric for how \"together\" the different values in the high_income column are.\n",
    "\n",
    "#### Data scientists commonly use a metric called entropy for this purpose. Entropy refers to disorder. The more \"mixed together\" 1s and 0s are, the higher the entropy. A data set consisting entirely of 1s in the high_income column would have low entropy.\n",
    "\n",
    "#### Entropy, which is not to be confused with entropy from physics, comes from information theory. Information theory is based on probability and statistics, and deals with the transmission, processing, utilization, and extraction of information. A key concept in information theory is the notion of a bit of information. One bit of information is one unit of information.\n",
    "\n",
    "#### We can represent a bit of information as a binary number because it either has the value 1 or 0. Suppose there's an equal probability of tomorrow being sunny (1) or not sunny (0). If I tell you that it will be sunny, I've given you one bit of information.\n",
    "\n",
    "#### We can also think of entropy in terms of information. If we flip a coin where both sides are heads, we know upfront that the result will be heads. We gain no new information by flipping the coin, so entropy is 0. On the other hand, if the coin has a heads side and a tails side, there's a 50% probability that it will land on either. Thus, flipping the coin gives us one bit of information -- which side the coin landed on.\n",
    "\n",
    "### Compute entropy\n",
    "\n",
    "#### Compute the entropy of the high_income column in the income dataframe, and assign the result to income_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n",
      "p_0: 0.7591904425539756\n",
      "p_1: 0.2408095574460244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7963839552022132"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "# We'll do the same calculation we did above, but in Python\n",
    "# Passing in 2 as the second parameter to math.log will take a base 2 log\n",
    "entropy = -(2/5 * math.log(2/5, 2) + 3/5 * math.log(3/5, 2))\n",
    "print(entropy)\n",
    "prob_0 = income[income[\"high_income\"] == 0].shape[0] / income.shape[0]\n",
    "prob_1 = income[income[\"high_income\"] == 1].shape[0] / income.shape[0]\n",
    "print('p_0: {}'.format(prob_0))\n",
    "print('p_1: {}'.format(prob_1))\n",
    "income_entropy = -(prob_0 * math.log(prob_0, 2) + prob_1 * math.log(prob_1, 2))\n",
    "income_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll need a way to go from computing entropy to figuring out which variable to split on. We can do this using information gain, which tells us which split will reduce entropy the most.\n",
    "\n",
    "#### We're computing information gain () for a given target variable (), as well as a given variable we want to split on ().\n",
    "\n",
    "#### To compute it, we first calculate the entropy for . Then, for each unique value  in the variable , we compute the number of rows in which  takes on the value , and divide it by the total number of rows. Next, we multiply the results by the entropy of the rows where  is . We add all of these subset entropies together, then subtract from the overall entropy to get information gain.\n",
    "\n",
    "#### Here's an alternate explanation. We're finding the entropy of each set post-split, weighting it by the number of items in each split, then subtracting from the current entropy. If the result is positive, we've lowered entropy with our split. The higher the result is, the more we've lowered entropy.\n",
    "\n",
    "#### One strategy for constructing trees is to create as many branches at each node as there are unique values for the variable we're splitting on. So if the variable has three or four values, we'd end up with three or four branches. This approach usually involves more complexity than it's worth and doesn't improve prediction accuracy, but it's worth knowing about.\n",
    "\n",
    "#### To simplify the calculation of information gain and make splits simpler, we won't do it for each unique value. We'll find the median for the variable we're splitting on instead. Any rows where the value of the variable is below the median will go to the left branch, and the rest of the rows will go to the right branch. To compute information gain, we'll only have to compute entropies for two subsets.\n",
    "\n",
    "### Compute information gain\n",
    "\n",
    "#### Compute the information gain for splitting on the age column of income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n",
      "0.17095059445466854\n"
     ]
    }
   ],
   "source": [
    "# compute the median of age.\n",
    "# assign anything less than or equal to the median to \n",
    "# the left branch, and anything greater than the median \n",
    "# to the right branch.\n",
    "# compute the information gain and assign it to age_information_gain\n",
    "import numpy\n",
    "\n",
    "def calc_entropy(column):\n",
    "    \"\"\"\n",
    "    Calculate entropy given a pandas series, list, or numpy array.\n",
    "    \"\"\"\n",
    "    # Compute the counts of each unique value in the column\n",
    "    counts = numpy.bincount(column)\n",
    "    # Divide by the total column length to get a probability\n",
    "    probabilities = counts / len(column)\n",
    "    \n",
    "    # Initialize the entropy to 0\n",
    "    entropy = 0\n",
    "    # Loop through the probabilities, and add each one to the total entropy\n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            entropy += prob * math.log(prob, 2)\n",
    "    \n",
    "    return -entropy\n",
    "\n",
    "# Verify that our function matches our answer from earlier\n",
    "entropy = calc_entropy([1,1,0,0,1])\n",
    "print(entropy)\n",
    "\n",
    "information_gain = entropy - ((.8 * calc_entropy([1,1,0,0])) + (.2 * calc_entropy([1])))\n",
    "print(information_gain)\n",
    "income_entropy = calc_entropy(income[\"high_income\"])\n",
    "\n",
    "median_age = income[\"age\"].median()\n",
    "\n",
    "left_split = income[income[\"age\"] <= median_age]\n",
    "right_split = income[income[\"age\"] > median_age]\n",
    "\n",
    "age_information_gain = income_entropy - ((left_split.shape[0] / income.shape[0]) * calc_entropy(left_split[\"high_income\"]) + ((right_split.shape[0] / income.shape[0]) * calc_entropy(right_split[\"high_income\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we know how to compute information gain, we can determine the best variable to split a node on. When we start our tree, we want to make an initial split. We'll find the variable to split on by calculating which split would have the highest information gain.\n",
    "\n",
    "#### Create a list called information_gains. It should contain, in order, the information gain from splitting on these columns: age, workclass, education_num, marital_status, occupation, relationship, race, sex, hours_per_week, native_country.\n",
    "\n",
    "#### Find the highest value in the information_gains list, and assign the corresponding column name to highest_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.047028661304691965\n",
      "Highest gain: education_num\n"
     ]
    }
   ],
   "source": [
    "def calc_information_gain(data, split_name, target_name):\n",
    "    \"\"\"\n",
    "    Calculate information gain given a data set, column to split on, and target\n",
    "    \"\"\"\n",
    "    # Calculate the original entropy\n",
    "    original_entropy = calc_entropy(data[target_name])\n",
    "    \n",
    "    # Find the median of the column we're splitting\n",
    "    column = data[split_name]\n",
    "    median = column.median()\n",
    "    \n",
    "    # Make two subsets of the data, based on the median\n",
    "    left_split = data[column <= median]\n",
    "    right_split = data[column > median]\n",
    "    \n",
    "    # Loop through the splits and calculate the subset entropies\n",
    "    to_subtract = 0\n",
    "    for subset in [left_split, right_split]:\n",
    "        prob = (subset.shape[0] / data.shape[0]) \n",
    "        to_subtract += prob * calc_entropy(subset[target_name])\n",
    "    \n",
    "    # Return information gain\n",
    "    return original_entropy - to_subtract\n",
    "\n",
    "# Verify that our answer is the same as on the last screen\n",
    "print(calc_information_gain(income, \"age\", \"high_income\"))\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "information_gains = []\n",
    "# Loop through and compute information gains\n",
    "for col in columns:\n",
    "    information_gain = calc_information_gain(income, col, \"high_income\")\n",
    "    information_gains.append(information_gain)\n",
    "\n",
    "# Find the name of the column with the highest gain\n",
    "highest_gain_index = information_gains.index(max(information_gains))\n",
    "highest_gain = columns[highest_gain_index]\n",
    "print('Highest gain: {}'.format(highest_gain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model\n",
    "\n",
    "#### Now we know how to make a single split. To construct an entire tree, we'll need to continue creating splits until the leaves only have a single class. We've been following the ID3 algorithm to construct decision trees. There are other algorithms like CART that use different measures for the split criterion. \n",
    "\n",
    "#### We'll use the ID3 Algorithm for constructing decision trees to accomplish this. This algorithm involves recursion and an understanding of time complexity. If you're unfamiliar with these topics, we suggest trying our Data Structures and Algorithms course. We also suggest learning about lambda functions through our command line course.\n",
    "\n",
    "#### In general, recursion is the process of splitting a large problem into smaller chunks. Recursive functions will call themselves, then combine the results into a final output.\n",
    "\n",
    "#### Building a tree is a perfect use case for recursive algorithms. At each node, we'll call a recursive function that will split the data into two branches. Each branch will lead to a node, and the function will call itself to build the tree out.\n",
    "\n",
    "#### We wrote functions to calculate entropy and information gain. We've loaded these functions in as calc_entropy() and calc_information_gain().\n",
    "\n",
    "#### Now we need a function that returns the name of the column we should use to split a data set. The function should take the name of the data set, the target column, and a list of columns we might want to split on as input.\n",
    "\n",
    "#### Write a function named find_best_column() that returns the name of a column to split the data on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_column(data, target_name, columns):\n",
    "    # Fill in the logic here to automatically find the column in columns to split on\n",
    "    # data is a dataframe\n",
    "    # target_name is the name of the target variable\n",
    "    # columns is a list of potential columns to split on\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use find_best_column() to find the best column on which to split income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'education_num'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A list of columns to potentially split income with\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "def find_best_column(data, target_name, columns):\n",
    "    information_gains = []\n",
    "    # Loop through and compute information gains\n",
    "    for col in columns:\n",
    "        information_gain = calc_information_gain(data, col, \"high_income\")\n",
    "        information_gains.append(information_gain)\n",
    "\n",
    "    # Find the name of the column with the highest gain\n",
    "    highest_gain_index = information_gains.index(max(information_gains))\n",
    "    highest_gain = columns[highest_gain_index]\n",
    "    return highest_gain\n",
    "\n",
    "income_split = find_best_column(income, \"high_income\", columns)\n",
    "income_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's build up to making the full id3() function by creating a simpler algorithm that we can extend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1s = []\n",
    "label_0s = []\n",
    "def id3(data, target, columns):\n",
    "    unique_targets = pandas.unique(data[target])\n",
    "\n",
    "    if len(unique_targets) == 1:\n",
    "        if 0 in unique_targets:\n",
    "            label_0s.append(0)\n",
    "        elif 1 in unique_targets:\n",
    "            label_1s.append(1)\n",
    "        return\n",
    "    \n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    \n",
    "    for split in [left_split, right_split]:\n",
    "        id3(split, target, columns)\n",
    "\n",
    "# Create the data set that we used in the example on the last screen\n",
    "data = pandas.DataFrame([\n",
    "    [0,20,0],\n",
    "    [0,60,2],\n",
    "    [0,40,1],\n",
    "    [1,25,1],\n",
    "    [1,35,2],\n",
    "    [1,55,1]\n",
    "    ])\n",
    "# Assign column names to the data\n",
    "data.columns = [\"high_income\", \"age\", \"marital_status\"]\n",
    "# Call the function on our data to set the counters properly\n",
    "id3(data, \"high_income\", [\"age\", \"marital_status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can store the entire tree, rather than the leaf labels only. We'll use nested dictionaries to do this. We can represent the root node with a dictionary, and branches with the keys left and right. We'll store the column we're splitting on as the key column, and the median value as the key median. Finally, we can store the label for a leaf as the key label. We'll also number each node as we go along using the number key.\n",
    "\n",
    "#### We're now passing the tree dictionary into our id3 function and setting some keys on it. One complexity is in how we're creating the nested dictionary. For the left split, we're adding a key to the tree dictionary that looks like this:\n",
    "\n",
    "- tree[\"left\"] = {}\n",
    "\n",
    "#### For the right side, we're adding:\n",
    "\n",
    "- tree[\"right\"] = {}\n",
    "\n",
    "#### Now that we've added this key, we're able to pass our new dictionary into the recursive call to id3(). While this new dictionary will be the dictionary for that specific node, it will be tied back to the parent dictionary (because it's a key of the original dictionary).\n",
    "\n",
    "#### This process will continue building up the nested dictionary. We'll be able to access the entire dictionary using the variable tree we define before the function. Think of each recursive call as building a piece of the tree, which we can then access after all of the functions have terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the correct label to the tree dictionary\n",
    "# assign the column and median keys to the tree dictionary\n",
    "# call the id3 function with the correct inputs: \n",
    "# id3(data, \"high_income\", [\"age\", \"marital_status\"], tree)\n",
    "# Create a dictionary to hold the tree  \n",
    "# It has to be outside of the function so we can access it later\n",
    "tree = {}\n",
    "\n",
    "# This list will let us number the nodes  \n",
    "# It has to be a list so we can access it inside the function\n",
    "nodes = []\n",
    "\n",
    "def id3(data, target, columns, tree):\n",
    "    unique_targets = pandas.unique(data[target])\n",
    "    \n",
    "    # Assign the number key to the node dictionary\n",
    "    nodes.append(len(nodes) + 1)\n",
    "    tree[\"number\"] = nodes[-1]\n",
    "\n",
    "    if len(unique_targets) == 1:\n",
    "        # Insert code here that assigns the \"label\" field to the node dictionary\n",
    "        return\n",
    "    \n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    # Insert code here that assigns the \"column\" and \"median\" fields to the node dictionary\n",
    "    \n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    split_dict = [[\"left\", left_split], [\"right\", right_split]]\n",
    "    \n",
    "    for name, split in split_dict:\n",
    "        tree[name] = {}\n",
    "        id3(split, target, columns, tree[name])\n",
    "\n",
    "# Call the function on our data to set the counters properly\n",
    "id3(data, \"high_income\", [\"age\", \"marital_status\"], tree)\n",
    "tree = {}\n",
    "nodes = []\n",
    "\n",
    "def id3(data, target, columns, tree):\n",
    "    unique_targets = pandas.unique(data[target])\n",
    "    nodes.append(len(nodes) + 1)\n",
    "    tree[\"number\"] = nodes[-1]\n",
    "\n",
    "    if len(unique_targets) == 1:\n",
    "        if 0 in unique_targets:\n",
    "            tree[\"label\"] = 0\n",
    "        elif 1 in unique_targets:\n",
    "            tree[\"label\"] = 1\n",
    "        return\n",
    "    \n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    tree[\"column\"] = best_column\n",
    "    tree[\"median\"] = column_median\n",
    "    \n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    split_dict = [[\"left\", left_split], [\"right\", right_split]]\n",
    "    \n",
    "    for name, split in split_dict:\n",
    "        tree[name] = {}\n",
    "        id3(split, target, columns, tree[name])\n",
    "\n",
    "\n",
    "id3(data, \"high_income\", [\"age\", \"marital_status\"], tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': 1,\n",
       " 'column': 'age',\n",
       " 'median': 37.5,\n",
       " 'left': {'number': 2,\n",
       "  'column': 'age',\n",
       "  'median': 25.0,\n",
       "  'left': {'number': 3,\n",
       "   'column': 'age',\n",
       "   'median': 22.5,\n",
       "   'left': {'number': 4, 'label': 0},\n",
       "   'right': {'number': 5, 'label': 1}},\n",
       "  'right': {'number': 6, 'label': 1}},\n",
       " 'right': {'number': 7,\n",
       "  'column': 'age',\n",
       "  'median': 55.0,\n",
       "  'left': {'number': 8,\n",
       "   'column': 'age',\n",
       "   'median': 47.5,\n",
       "   'left': {'number': 9, 'label': 0},\n",
       "   'right': {'number': 10, 'label': 1}},\n",
       "  'right': {'number': 11, 'label': 0}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tree dictionary shows all of the relevant information, but it doesn't look very nice. We can fix its appearance by printing it out in a nicer format.\n",
    "\n",
    "#### To do this, we'll need to recursively iterate through our tree dictionary. Any dictionary that has a label key is a leaf. Whenever we find one, we'll print out the label. Otherwise, we'll loop through the tree's left and right keys and recursively call the same function.\n",
    "\n",
    "#### We also need to keep track of a depth variable. This variable will allow us to use indentation to indicate the order of the nodes. Before we print anything out, we'll prefix it with the number of spaces corresponding to the depth variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age > 37.5\n",
      "age > 37.5\n",
      "    age > 25.0\n",
      "        age > 22.5\n",
      "            Leaf: Label 0\n",
      "            Leaf: Label 1\n",
      "        Leaf: Label 1\n",
      "    age > 55.0\n",
      "        age > 47.5\n",
      "            Leaf: Label 0\n",
      "            Leaf: Label 1\n",
      "        Leaf: Label 0\n"
     ]
    }
   ],
   "source": [
    "def print_with_depth(string, depth):\n",
    "    # Add space before a string\n",
    "    prefix = \"    \" * depth\n",
    "    # Print a string, and indent it appropriately\n",
    "    print(\"{0}{1}\".format(prefix, string))\n",
    "    \n",
    "    \n",
    "def print_node(tree, depth):\n",
    "    # Check for the presence of \"label\" in the tree\n",
    "    if \"label\" in tree:\n",
    "        # If found, then this is a leaf, so print it and return\n",
    "        print_with_depth(\"Leaf: Label {0}\".format(tree[\"label\"]), depth)\n",
    "        # This is critical -- without it, you'll get infinite recursion\n",
    "        return\n",
    "    # Print information about what the node is splitting on\n",
    "    print_with_depth(\"{0} > {1}\".format(tree[\"column\"], tree[\"median\"]), depth)\n",
    "    \n",
    "    # Create a list of tree branches\n",
    "    branches = [tree[\"left\"], tree[\"right\"]]\n",
    "        \n",
    "    # Insert code here to recursively call print_node on each branch\n",
    "    # Don't forget to increment depth when you pass it in\n",
    "\n",
    "print_node(tree, 0)\n",
    "def print_node(tree, depth):\n",
    "    if \"label\" in tree:\n",
    "        print_with_depth(\"Leaf: Label {0}\".format(tree[\"label\"]), depth)\n",
    "        return\n",
    "    print_with_depth(\"{0} > {1}\".format(tree[\"column\"], tree[\"median\"]), depth)\n",
    "    for branch in [tree[\"left\"], tree[\"right\"]]:\n",
    "        print_node(branch, depth+1)\n",
    "\n",
    "print_node(tree, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's say we want to predict the following row:\n",
    "    age    marital_status\n",
    "    50     1\n",
    "#### First, we'd split on age > 37.5 and go to the right. Then, we'd split on age > 55.0 and go to the left. Then, we'd split on age > 47.5 and go to the right. We'd end up predicting a 1 for high_income.\n",
    "\n",
    "#### Making predictions with such a small tree is fairly straightforward, but what if we want to use the entire income dataframe? We wouldn't be able to eyeball predictions; we'd want an automated way to do this instead.\n",
    "\n",
    "#### Let's write a function that makes predictions automatically. All we need to do is follow the split points we've already defined with a new row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def predict(tree, row):\n",
    "    if \"label\" in tree:\n",
    "        return tree[\"label\"]\n",
    "    \n",
    "    column = tree[\"column\"]\n",
    "    median = tree[\"median\"]\n",
    "    if row[column] <= median:\n",
    "        return predict(tree[\"left\"], row)\n",
    "    else:\n",
    "        return predict(tree[\"right\"], row)\n",
    "\n",
    "print(predict(tree, data.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we can make a prediction for a single row, we can write a function that makes predictions for multiple rows simultanously.\n",
    "\n",
    "#### To do this, we'll use the apply() method on pandas dataframes to apply a function across each row. You can read more about the function in the pandas documentation. You'll need to pass in the axis=1 argument to apply the function to each row. This method will return a dataframe.\n",
    "\n",
    "#### You can use the apply() method along with lambda functions to apply the predict() function to each row of new_data.\n",
    "\n",
    "#### Create a function named batch_predict() that takes two parameters, tree and df.\n",
    "\n",
    "#### It should use the apply() method to apply the predict() function across each row of df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pandas.DataFrame([\n",
    "    [40,0],\n",
    "    [20,2],\n",
    "    [80,1],\n",
    "    [15,1],\n",
    "    [27,2],\n",
    "    [38,1]\n",
    "    ])\n",
    "# Assign column names to the data\n",
    "new_data.columns = [\"age\", \"marital_status\"]\n",
    "\n",
    "def batch_predict(tree, df):\n",
    "    # Insert your code here\n",
    "    pass\n",
    "\n",
    "predictions = batch_predict(tree, new_data)\n",
    "def batch_predict(tree, df):\n",
    "    return df.apply(lambda x: predict(tree, x), axis=1)\n",
    "\n",
    "predictions = batch_predict(tree, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "5    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We learned how to create a full decision tree model, print the results, and use the tree to make predictions. We applied a modified version of the ID3 algorithm on a small data set for clarity.\n",
    "\n",
    "#### Next, we'll learn about when to use decision trees, and how to use them most effectively. We can use the scikit-learn package to fit a decision tree. \n",
    "\n",
    "#### We use the DecisionTreeClassifier class for classification problems, and DecisionTreeRegressor for regression problems. The sklearn.tree package includes both of these classes.\n",
    "\n",
    "#### In this case, we're predicting a binary outcome, so we'll use a classifier.\n",
    "\n",
    "#### The first step is to train the classifier on the data. We'll use the fit method on a classifier to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# A list of columns to train with\n",
    "# We've already converted all columns to numeric\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "# Instantiate the classifier\n",
    "# Set random_state to 1 to make sure the results are consistent\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# We've already loaded the variable \"income,\" which contains all of the income data\n",
    "clf.fit(income[columns], income[\"high_income\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we've fit a model, we can make predictions. We'll want to split our data into training and testing sets first. If we don't, we'll be making predictions on the same data that we train our algorithm with. This leads to overfitting, and will make our error appear lower than it is.\n",
    "\n",
    "#### We can avoid overfitting by always making predictions and evaluating error on data that we haven't trained our algorithm with. This will show us when we're overfitting by giving us a realistic error on data that the algorithm hasn't seen before.\n",
    "\n",
    "#### We can split the data by shuffling the order of the dataframe, then selecting certain rows to include in the training set, and certain rows to include in the testing set.\n",
    "\n",
    "#### In this case, we'll make 80% of our rows training data, and the rest testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "\n",
    "# Set a random seed so the shuffle is the same every time\n",
    "numpy.random.seed(1)\n",
    "\n",
    "# Shuffle the rows  \n",
    "# This permutes the index randomly using numpy.random.permutation\n",
    "# Then, it reindexes the dataframe with the result\n",
    "# The net effect is to put the rows into random order\n",
    "income = income.reindex(numpy.random.permutation(income.index))\n",
    "\n",
    "train_max_row = math.floor(income.shape[0] * .8)\n",
    "train = income.iloc[:train_max_row]\n",
    "test = income.iloc[train_max_row:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC ranges from 0 to 1, so it's ideal for binary classification. The higher the AUC, the more accurate our predictions.\n",
    "\n",
    "#### We can compute AUC with the roc_auc_score function from sklearn.metrics. This function takes in two parameters:\n",
    "\n",
    "- y_true: true labels\n",
    "- y_score: predicted labels\n",
    "\n",
    "#### It then calculates and returns the AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6971330661993492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "error = roc_auc_score(test[\"high_income\"], predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The AUC for the predictions on the testing set is about .694. Let's compare this against the AUC for predictions on the training set to see if the model is overfitting.\n",
    "\n",
    "#### It's normal for the model to predict the training set better than the testing set. After all, it has full knowledge of that data and the outcomes. However, if the AUC between training set predictions and actual values is significantly higher than the AUC between test set predictions and actual values, it's a sign that the model may be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9471244501437455\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(train[columns])\n",
    "print(roc_auc_score(train[\"high_income\"], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfittng\n",
    "\n",
    "#### Our AUC on the training set was .947, and the AUC on the test set was .694. There's no hard and fast rule on when overfitting is occurring, but our model is predicting the training set much better than the test set. Splitting the data into training and testing sets doesn't prevent overfitting -- it just helps us detect and fix it.\n",
    "\n",
    "#### Based on our AUC measurements, it appears that we are in fact overfitting.\n",
    "\n",
    "#### Trees overfit when they have too much depth and make overly complex rules that match the training data, but aren't able to generalize well to new data. This may seem to be a strange principle at first, but the deeper a tree is, the worse it typically performs on new data.\n",
    "\n",
    "#### There are three main ways to combat overfitting:\n",
    "\n",
    "- \"Prune\" the tree after we build it to remove unnecessary leaves.\n",
    "- Use ensembling to blend the predictions of many trees.\n",
    "- Restrict the depth of the tree while we're building it.\n",
    "\n",
    "#### While we'll explore all of these, we'll look at the third method first.\n",
    "\n",
    "#### Limiting tree depth during the building process will result in more general rules. This prevents the tree from overfitting.\n",
    "\n",
    "#### We can restrict tree depth by adding a few parameters when we initialize the DecisionTreeClassifier class:\n",
    "\n",
    "- max_depth - Globally restricts how deep the tree can go\n",
    "- min_samples_split - The minimum number of rows a node should have before it can be split; if this is set to 2, for example, then nodes with 2 rows won't be split, and will become leaves instead\n",
    "- min_samples_leaf - The minimum number of rows a leaf must have\n",
    "- min_weight_fraction_leaf - The fraction of input rows a leaf must have\n",
    "- max_leaf_nodes - The maximum number of total leaves; this will cap the count of leaf nodes as the tree is being built\n",
    "\n",
    "#### Some of these parameters aren't compatible, however. For example, we can't use max_depth and max_leaf_nodes together.\n",
    "\n",
    "#### Now that we know what to tweak, let's improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7119952337831613\n",
      "0.8426415759234083\n"
     ]
    }
   ],
   "source": [
    "# Set min_samples_split to 13 when creating the \n",
    "# DecisionTreeClassifier.\n",
    "# Make predictions on the training set, compute the \n",
    "# AUC, and assign it to train_auc.\n",
    "# Make predictions on the test set, compute the AUC, \n",
    "# and assign it to test_auc.\n",
    "# Decision trees model from the last screen\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf = DecisionTreeClassifier(min_samples_split=13, random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(test[\"high_income\"], predictions)\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By setting min_samples_split to 13, we managed to boost the test AUC from .694 to .700. The training set AUC decreased from .947 to .843, showing that the model we built was less overfit to the training set than before.\n",
    "\n",
    "#### Let's play around with parameters some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6971330661993492\n",
      "0.9471244501437455\n",
      "0.736437254165992\n",
      "0.7423073733990373\n"
     ]
    }
   ],
   "source": [
    "# Set max_depth to 7 and min_samples_split to 13 when \n",
    "# creating the DecisionTreeClassifier.\n",
    "# Make predictions on the training set, compute \n",
    "# the AUC, and assign it to train_auc.\n",
    "# Make predictions on the test set, compute the \n",
    "# AUC, and assign it to test_auc.\n",
    "# The first decision trees model we trained and tested\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(test[\"high_income\"], predictions)\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)\n",
    "clf = DecisionTreeClassifier(random_state=1, min_samples_split=13, max_depth=7)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(test[\"high_income\"], predictions)\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We just improved the AUC again! The test set AUC increased to .744, while the training set AUC decreased to .748. We aren't overfitting anymore because both AUC values are about the same. Let's tweak the parameters more aggressively and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6971330661993492\n",
      "0.9471244501437455\n",
      "0.6801979250344399\n",
      "0.6916321420555511\n"
     ]
    }
   ],
   "source": [
    "# Set max_depth to 2 and min_samples_split to \n",
    "# 100 when creating the DecisionTreeClassifier.\n",
    "# Make predictions on the training set, compute the \n",
    "# AUC, and assign it to train_auc.\n",
    "# Make predictions on the test set, compute the \n",
    "# AUC, and assign it to test_auc.\n",
    "# The first decision tree model we trained and tested\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(test[\"high_income\"], predictions)\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)\n",
    "clf = DecisionTreeClassifier(random_state=1, min_samples_split=100, max_depth=2)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(test[\"high_income\"], predictions)\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "\n",
    "#### Our accuracy went down  relative to results before. This is because we're now underfitting. Underfitting is what occurs when our model is too simple to explain the relationships between the variables.\n",
    "\n",
    "#### By artificially restricting the depth of our tree, we prevent it from creating a model that's complex enough to correctly categorize some of the rows. If we don't perform the artificial restrictions, however, the tree becomes too complex, fits quirks in the data that only exist in the training set, and doesn't generalize to new data.\n",
    "\n",
    "#### This is known as the bias-variance tradeoff. Imagine that we take a random sample of training data and create many models. If the models' predictions for the same row are far apart from each other, we have high variance. Imagine this time that we take a random sample of the training data and create many models. If the models' predictions for the same row are close together but far from the actual value, then we have high bias.\n",
    "\n",
    "#### High bias can cause underfitting -- if a model is consistently failing to predict the correct value, it may be that it's too simple to model the data faithfully.\n",
    "\n",
    "#### High variance can cause overfitting. If a model varies its predictions significantly based on small changes in the input data, then it's likely fitting itself to quirks in the training data, rather than making a generalizable model.\n",
    "\n",
    "#### We call this the bias-variance tradeoff because decreasing one characteristic will usually increase the other. This is a limitation of all machine learning algorithms.\n",
    "\n",
    "#### Decision trees typically suffer from high variance. The entire structure of a decision tree can change if we make a minor alteration to its training data. By restricting the depth of the tree, we increase the bias and decrease the variance. If we restrict the depth too much, we increase bias to the point where it will underfit.\n",
    "\n",
    "#### We can induce variance and see what happens with a decision tree. To introduce noise into the data, we'll add a column of random values. A model with high variance (like a decision tree) will pick up on this noise, and overfit to it. This is because models with high variance are very sensitive to small changes in input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7033070734546627\n",
      "0.9750761614350801\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(1)\n",
    "\n",
    "# Generate a column containing random numbers from 0 to 4\n",
    "income[\"noise\"] = numpy.random.randint(4, size=income.shape[0])\n",
    "\n",
    "# Adjust \"columns\" to include the noise column\n",
    "columns = [\"noise\", \"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "# Make new train and test sets\n",
    "train_max_row = math.floor(income.shape[0] * .8)\n",
    "train = income.iloc[:train_max_row]\n",
    "test = income.iloc[train_max_row:]\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(test[\"high_income\"], predictions)\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The random noise column causes significant overfitting. Our test set accuracy decreases to .703, and our training set accuracy increases to .975.\n",
    "\n",
    "#### One way to prevent overfitting is to block the tree from growing beyond a certain depth (we tried this before). Another technique is called pruning. Pruning involves building a full tree, and then removing the leaves that don't add to prediction accuracy. Pruning prevents a model from becoming overly complex. It can result in a simpler model that has higher accuracy on the testing set.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "#### Let's go over the main advantages and disadvantages of using decision trees. The main advantages of using decision trees is that they're:\n",
    "\n",
    "- Easy to interpret\n",
    "- Relatively fast to fit and make predictions\n",
    "- Able to handle multiple types of data\n",
    "- Able to pick up nonlinearities in data, and usually fairly accurate\n",
    "\n",
    "#### The main disadvantage of using decision trees is their tendency to overfit.\n",
    "\n",
    "#### Decision trees are a good choice for tasks where it's important to be able to interpret and convey why the algorithm is doing what it's doing.\n",
    "\n",
    "#### The most powerful way to reduce decision tree overfitting is to create ensembles of trees. The random forest algorithm is a popular choice for doing this. In cases where prediction accuracy is the most important consideration, random forests usually perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
